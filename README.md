# JAX/Flax GPT-2 Theory, Implementation, and Inference

This notebook provides a JAX/Flax implementation of GPT-2 architecture using pretrained Hugging Face weights for inference. It includes a detailed explanation of the theory and implementation.

## Key Components
- Self-Attention
- Causal Masking
- Multi-Head Attention
- Feed Forward Network
- Residual Connections & Layer Normalization
- Text Generation Methods
