# JAX/Flax GPT-2 Theory, Implementation, and Inference

This notebook provides a JAX/Flax implementation of GPT-2 architecture using pretrained Hugging Face weights for inference. It includes a detailed explanation of the theory and implementation.

## Key Components
- Self-Attention
- Causal Masking
- Multi-Head Attention
- Feed Forward Network
- Residual Connections & Layer Normalization
- Text Generation Methods

## nbviewer link
https://nbviewer.org/github/s4piru/GPT2-JAX/blob/main/gpt2_jax.ipynb
