{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNVkklgcE0O6GVigPQJN2fn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s4piru/GPT2-JAX/blob/main/gpt2_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2 (Decoder-only Transformer)\n",
        "\n",
        "Some transformers have both encoder and ecoder such as machine translation model. However, GPT-2 is an autoregressive language model that uses only decoder. Since GPT-2 focuses on generating the next token probabilistically based on the given context, it does not need encoder and only uses decoder.\n",
        "\n",
        "## Self-Attention\n",
        "\n",
        "Self-attention is the core mechanism of Transformers. In self-attention, each word in a sequence calculates its relevance to other words and integrates information accordingly.\n",
        "\n",
        "* Specifically, for each input word vector $\\mathbf{x}$, three vectors are generated: Query, Key, and Value. These are computed using learnable weight matrices:  \n",
        "  $\\mathbf{q} = \\mathbf{x} \\mathbf{W}^Q$,  \n",
        "  $\\mathbf{k} = \\mathbf{x} \\mathbf{W}^K$,  \n",
        "  $\\mathbf{v} = \\mathbf{x} \\mathbf{W}^V$.\n",
        "\n",
        "* Calculate attention scores: The dot product of a Query vector $\\mathbf{q}$ and a Key vector $\\mathbf{k}$ determines how much attention should be paid to each word. A softmax function normalizes these scores.\n",
        "\n",
        "* Weight/Aggregate Values: The obtained attention weights are applied to the Value vectors, and their weighted sum creates a new word representation. This process updates a words representation to reflect related words information.\n",
        "\n",
        "If the word \"run\" assigns high attention weight to \"dog,\" it indicates the information that \"the one running is the dog.\"\n",
        "\n",
        "## Causal Mask\n",
        "\n",
        "In decoder's self-attention, causal mask is essential to prevent information leakage from the future. Specifically, at time step $t$, tokens must not see words from $t$ onward. This is done by adding a mask value of $-\\infty$ to invalid positions in the attention score matrix. This ensures that, after applying softmax, future words receive zero attention, allowing Transformer decoder to generate text sequentially from left to right.\n",
        "\n",
        "## Positional Encoding\n",
        "\n",
        "Self-attention learns relationships between words regardless of order. However, without explicitly encoding position, it cannot distinguish word order and require explicit position encoding.\n",
        "\n",
        "* Fixed positional encoding: Uses sin and cos waves at different frequencies to encode position. Close positions have similar wave patterns, while distant positions show greater phase shifts.\n",
        "\n",
        "* Learnable positional encoding: A later approach allows the model to learn position encodings itself.\n",
        "\n",
        "* Relative positional encoding: Some models, like BERT extensions, use relative distance-based encoding.\n",
        "\n",
        "## Multi-Head Attention\n",
        "\n",
        "Instead of a single attention mechanism, Transformers use multiple parallel attention heads. This is called multi-head attention. Each head learns different weight matrices $\\mathbf{W}_i^Q, \\mathbf{W}_i^K, \\mathbf{W}_i^V$ and captures different aspects of word relationships.\n",
        "\n",
        "GPT-2 Small has 12 attention heads, each with different weight matrices. Each head projects the input 768 dimensions hidden vector into a 64-dimensional space. These projections are then combined to capture diverse relationships.\n",
        "\n",
        "## Feed Forward Network\n",
        "\n",
        "In each Transformer layer, after multi-head self-attention, a Feed Forward Networkis applied. The FFN captures nonlinear relationships.\n",
        "\n",
        "$\\mathbf{FFN}(\\mathbf{h}) = \\mathbf{GELU}(\\mathbf{h}W_1 + \\mathbf{b}_1) W_2 + \\mathbf{b}_2$\n",
        "\n",
        "FFN applies transformations to enhance word-level features. First, it temporarily expands the hidden dimension (GPT-2 is from 768 to 3072), applies a non-linear GELU activation, and then reduces it back to the original size.\n",
        "\n",
        "## LayerNorm\n",
        "\n",
        "Standard Transformers apply Layer Normalization after each sublayer. However, GPT-2 uses a Pre-LN architecture, where LayerNorm is applied before sublayers. LayerNorm normalizes vector elements to zero mean and unit variance per sample, stabilizing training by preventing gradient vanishing or divergence.\n",
        "\n",
        "## Residual Connections\n",
        "\n",
        "In deep neural networks like Transformers, deeper layers improve representation power. However, deeper networks also face gradient vanishing issues, making it harder for early layers to learn. Adding residual connections helps preserve gradients.\n",
        "\n",
        "If $\\mathbf{X}$ is the input to a layer, and $\\mathbf{Sublayer}(\\mathbf{X})$ represents its processing, the residual connection transforms it as\n",
        "\n",
        "$\\mathbf{Y} = \\mathbf{X} + \\mathbf{Sublayer}(\\mathbf{X})\\$\n",
        "\n",
        "This ensures that at least the input gradient is maintained, preventing excessively small gradients and enabling stable training."
      ],
      "metadata": {
        "id": "w0NVfy3bDx7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation Methods\n",
        "\n",
        "### 1. Greedy Search\n",
        "Greedy Search is the simplest way to generate text. It selects the token with the highest probability from the softmax distribution when generating the next word. At each time step $t$, it picks the most probable next word.\n",
        "\n",
        "**Pros**\n",
        "* It has the lowest computational cost, making it suitable for real-time inference.\n",
        "* It is deterministic, allowing high reproducibility.\n",
        "\n",
        "**Cons**\n",
        "* It easily gets stuck in local optima, leading to repetitive text generation.\n",
        "\n",
        "### 2. Temperature Sampling\n",
        "Temperature Sampling is a method that randomly selects the next word based on probability distribution, adjusting the softmax probabilities with a temperature parameter to control diversity. A lower temperature makes high-probability tokens more likely to be chosen, while a higher temperature flattens the probabilities, allowing for more diverse word selection.\n",
        "\n",
        "**Pros**\n",
        "* It allows control over randomness, producing outputs with a balanced level of diversity.\n",
        "\n",
        "**Cons**\n",
        "* If the temperature is too high, low-probability words may be selected, increasing the risk of meaningless text.\n",
        "* If the temperature is too low, the output becomes similar to Greedy Search.\n",
        "\n",
        "### 3. Top-k Sampling\n",
        "Top-k Sampling considers only the top $k$ most probable tokens, normalizes their probabilities, and samples from them.\n",
        "\n",
        "**Pros**\n",
        "* It allows control over randomness.\n",
        "* Prevents selecting semantically inappropriate words.\n",
        "\n",
        "**Cons**\n",
        "* If $k$ is too small, it behaves similarly to Greedy Search, reducing diversity.\n",
        "* If $k$ is too large, it includes low-probability tokens, which may reduce coherence.\n",
        "* Choosing an appropriate $k$ value is difficult.\n",
        "\n",
        "### 4. Top-p Sampling\n",
        "Top-p Sampling selects the smallest set of words whose cumulative probability exceeds $p$, and samples from them.\n",
        "\n",
        "**Pros**\n",
        "* Unlike Top-k, this method dynamically adjusts the number of candidates based on the probability distribution.\n",
        "\n",
        "**Cons**\n",
        "* Choosing an appropriate $p$ value is difficult.\n",
        "\n",
        "### 5. Beam Search\n",
        "Beam Search explores multiple candidate sequences in parallel and selects the one with the highest overall probability. Unlike Greedy Search, which selects the best local word at each step, Beam Search expands multiple candidates based on the beam width and considers future words.\n",
        "\n",
        "**Pros**\n",
        "* It finds more contextually probable sequences compared to Greedy Search.\n",
        "* It is deterministic, allowing high reproducibility.\n",
        "\n",
        "**Cons**\n",
        "* A larger beam width increases computational cost.\n",
        "* All beams may converge to similar results.\n",
        "* Since it maximizes cumulative probability, shorter sentences may be preferred over longer ones.\n"
      ],
      "metadata": {
        "id": "Wx4PYADUrrpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the GPT-2 architecture using JAX/Flax and apply pretrained parameters from Hugging Face."
      ],
      "metadata": {
        "id": "TUZHrmT2flSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install flax transformers"
      ],
      "metadata": {
        "id": "wtpjEOp_fQsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "from flax.core.frozen_dict import freeze, unfreeze\n",
        "from flax.linen.attention import dot_product_attention\n",
        "from transformers import FlaxGPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "VQfuIUzmf3we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a custom implementation of a dense fully-connected layer similar to Flax's nn.Dense.\n",
        "class DenseT(nn.Module):\n",
        "    features: int\n",
        "    use_bias: bool = True\n",
        "    kernel_init: any = nn.initializers.lecun_normal()\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        in_features = inputs.shape[-1]\n",
        "        kernel = self.param(\"kernel\", self.kernel_init, (in_features, self.features))\n",
        "        y = jnp.dot(inputs, kernel)\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\"bias\", nn.initializers.zeros, (self.features,))\n",
        "            y = y + bias\n",
        "        return y\n",
        "\n",
        "# Applying a single linear transformation to the input x in order to generate a concatenated tensor containing the query, key, and value vectors.\n",
        "# Reshaping the tensor from (B, T, C) to (B, T, n_head, head_dim) to facilitate multi-head attention via the dot_product_attention function.\n",
        "# After computing the attention, the output is reshaped back to (B, T, C) and processed by a projection layer (c_proj).\n",
        "# A causal mask, implemented as a lower triangular matrix of shape (T, T).\n",
        "class GPT2SelfAttention(nn.Module):\n",
        "    n_embd: int\n",
        "    n_head: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, mask=None, deterministic=True):\n",
        "        B, T, C = x.shape\n",
        "        head_dim = C // self.n_head\n",
        "\n",
        "        # GPT-2 parameters derived from PyTorch, the weight for the concatenated query, key, and value transformation is often stored with shape (3*C, C).\n",
        "        # However, Flax's Dense layer expects parameters of shape (C, 3*C). Therefore, apply transpose during parameter initialization.\n",
        "        x_3c = DenseT(3 * C, use_bias=True, name=\"c_attn\")(x)\n",
        "        x_3c = x_3c.reshape(B, T, 3, C)\n",
        "        q, k, v = jnp.split(x_3c, 3, axis=2)\n",
        "        q = jnp.squeeze(q, axis=2)\n",
        "        k = jnp.squeeze(k, axis=2)\n",
        "        v = jnp.squeeze(v, axis=2)\n",
        "\n",
        "        # Multi-head splitting: Reshape query, key, and value tensors to (B, T, n_head, head_dim)\n",
        "        q = q.reshape(B, T, self.n_head, head_dim)\n",
        "        k = k.reshape(B, T, self.n_head, head_dim)\n",
        "        v = v.reshape(B, T, self.n_head, head_dim)\n",
        "\n",
        "        # Causal mask: Expand its dimensions to (B, n_head, T, T) and create a bias tensor.\n",
        "        # Positions disallowed by the mask are set to a very negative value (-1e10), effectively blocking attention.\n",
        "        if mask is not None:\n",
        "            expanded_mask = jnp.broadcast_to(mask, (B, self.n_head, T, T))\n",
        "            bias = jnp.where(expanded_mask, 0.0, -1e10)\n",
        "        else:\n",
        "            bias = None\n",
        "\n",
        "        # Compute dot-product attention using the queries, keys, values, and bias.\n",
        "        # The dropout rate is applied only when not in deterministic mode.\n",
        "        attn_out = dot_product_attention(\n",
        "            query=q,\n",
        "            key=k,\n",
        "            value=v,\n",
        "            bias=bias,\n",
        "            dropout_rate=self.dropout_rate if not deterministic else 0.0,\n",
        "            deterministic=deterministic,\n",
        "        )\n",
        "        # Reshape the attention output back to the original shape (B, T, C).\n",
        "        attn_out = attn_out.reshape(B, T, C)\n",
        "\n",
        "        # Apply a final linear projection and transpose.\n",
        "        out = nn.Dense(C, use_bias=True, name=\"c_proj\")(attn_out)\n",
        "        return out\n",
        "\n",
        "# Expanding the input dimensionality to 4*C via a fully-connected layer.\n",
        "# Applying the GELU activation function.\n",
        "# Using dropout for regularization.\n",
        "# Projecting the result back to the original dimension (C) with another fully-connected layer.\n",
        "class FeedForward(nn.Module):\n",
        "    n_embd: int\n",
        "    dropout_rate: float = 0.1\n",
        "    name: str = \"ffn\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic=True):\n",
        "        hidden = DenseT(4 * self.n_embd, use_bias=True, name=\"fc\")(x)\n",
        "        hidden = nn.gelu(hidden)\n",
        "        hidden = nn.Dropout(rate=self.dropout_rate)(hidden, deterministic=deterministic)\n",
        "        out = DenseT(self.n_embd, use_bias=True, name=\"proj\")(hidden)\n",
        "        return out\n",
        "\n",
        "# The output is computed as the element-wise sum of the token and positional embeddings.\n",
        "class GPT2Embed(nn.Module):\n",
        "    vocab_size: int\n",
        "    max_length: int\n",
        "    n_embd: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.token_embed = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embd)\n",
        "        self.pos_embed = self.param(\n",
        "            \"pos_embedding\",\n",
        "            nn.initializers.normal(stddev=0.02),\n",
        "            (self.max_length, self.n_embd),\n",
        "        )\n",
        "\n",
        "    def __call__(self, input_ids):\n",
        "        B, T = input_ids.shape\n",
        "        # Compute token embeddings; the resulting tensor: (B, T, n_embd).\n",
        "        token_emb = self.token_embed(input_ids)\n",
        "        # Slice the positional embeddings to match the sequence length T: (T, n_embd).\n",
        "        pos_emb = self.pos_embed[:T, :]\n",
        "        # Expand dimensions of positional embeddings to (1, T, n_embd) for broadcasting.\n",
        "        pos_emb = jnp.expand_dims(pos_emb, axis=0)\n",
        "        # Combine token and positional embeddings.\n",
        "        emb_out = token_emb + pos_emb\n",
        "        return emb_out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    n_embd: int\n",
        "    n_head: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, mask=None, deterministic=True):\n",
        "        # Self-Attention Sub-layer\n",
        "        residual = x  # Save input for the residual (skip) connection.\n",
        "        x = nn.LayerNorm(epsilon=1e-5)(x)  # Apply LayerNorm before self-attention.\n",
        "        x = GPT2SelfAttention(\n",
        "            n_embd=self.n_embd,\n",
        "            n_head=self.n_head,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            name=\"attn\"\n",
        "        )(x, mask=mask, deterministic=deterministic)\n",
        "        x = residual + x  # Add the residual connection.\n",
        "\n",
        "        # Feed-Forward (MLP) Sub-layer:\n",
        "        residual = x  # Save current tensor for the residual connection.\n",
        "        x = nn.LayerNorm(epsilon=1e-5)(x)  # Apply LayerNorm before the MLP.\n",
        "        x = FeedForward(\n",
        "            self.n_embd,\n",
        "            self.dropout_rate,\n",
        "            name=\"ffn\"\n",
        "        )(x, deterministic=deterministic)\n",
        "        x = residual + x  # Add the residual connection.\n",
        "        return x\n",
        "\n",
        "class GPT2LMModel(nn.Module):\n",
        "    vocab_size: int\n",
        "    max_length: int\n",
        "    n_embd: int\n",
        "    n_head: int\n",
        "    n_layer: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize the embedding module.\n",
        "        self.embed = GPT2Embed(\n",
        "            vocab_size=self.vocab_size,\n",
        "            max_length=self.max_length,\n",
        "            n_embd=self.n_embd,\n",
        "            name=\"embed\"\n",
        "        )\n",
        "        # Create a list of Transformer blocks.\n",
        "        self.blocks = [\n",
        "            TransformerBlock(\n",
        "                n_embd=self.n_embd,\n",
        "                n_head=self.n_head,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "                name=f\"block_{i}\"\n",
        "            )\n",
        "            for i in range(self.n_layer)\n",
        "        ]\n",
        "        # Final LayerNorm applied after the Transformer blocks.\n",
        "        self.ln_f = nn.LayerNorm(epsilon=1e-5)\n",
        "        # LM Head: A fully-connected layer that projects the hidden state to the vocabulary size.\n",
        "        # Weight tying is performed by later setting lm_head.kernel equal to the transpose of token_embed.embedding.\n",
        "        self.lm_head = nn.Dense(self.vocab_size, use_bias=False, name=\"lm_head\")\n",
        "\n",
        "    def __call__(self, input_ids, deterministic=True):\n",
        "        B, T = input_ids.shape\n",
        "        x = self.embed(input_ids)\n",
        "        # Create a causal mask: generate a lower triangular matrix (T, T) to prevent attention to future tokens.\n",
        "        causal_mask = jnp.tril(jnp.ones((T, T), dtype=bool))\n",
        "        # Expand the mask dimensions to (B, 1, T, T) for broadcasting over batches and attention heads.\n",
        "        causal_mask = causal_mask[None, None, :, :]\n",
        "        causal_mask = jnp.broadcast_to(causal_mask, (B, 1, T, T))\n",
        "\n",
        "        # Pass the input through each Transformer block with the causal mask applied.\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=causal_mask, deterministic=deterministic)\n",
        "\n",
        "        x = self.ln_f(x)  # Apply the final LayerNorm.\n",
        "        logits = self.lm_head(x)  # Project to vocabulary size.\n",
        "        return logits\n",
        "\n",
        "    def forward_intermediate(self, input_ids, deterministic=True):\n",
        "        \"\"\"\n",
        "        Extract intermediate outputs (hidden states) from each layer.\n",
        "        This function is intended for debugging and comparison purposes.\n",
        "        \"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        x = self.embed(input_ids)\n",
        "        hidden_states = [x]  # Store the embedding layer output.\n",
        "        # Create a causal mask.\n",
        "        causal_mask = jnp.tril(jnp.ones((T, T), dtype=bool))[None, None, :, :]\n",
        "        causal_mask = jnp.broadcast_to(causal_mask, (B, 1, T, T))\n",
        "\n",
        "        # Collect the output of each Transformer block.\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask=causal_mask, deterministic=deterministic)\n",
        "            hidden_states.append(x)\n",
        "        x = self.ln_f(x)  # Apply the final LayerNorm.\n",
        "        hidden_states[-1] = x  # Replace the last hidden state with the post-LayerNorm output.\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Parameter Conversion:\n",
        "# Transpose c_attn weights from (3*C, C) to (C, 3*C).\n",
        "# Transpose c_proj weights.\n",
        "# Transpose c_fc weights from (4*C, C) to (C, 4*C) and c_proj weights from (C, 4*C) to (4*C, C).\n",
        "def convert_hf_params_to_my_model(hf_params, my_params, n_layer=12):\n",
        "    mp = unfreeze(my_params)\n",
        "\n",
        "    # Copy token embeddings with shape (vocab_size, n_embd)\n",
        "    mp[\"embed\"][\"token_embed\"][\"embedding\"] = hf_params[\"transformer\"][\"wte\"][\"embedding\"]\n",
        "    # Copy positional embeddings with shape (max_length, n_embd)\n",
        "    mp[\"embed\"][\"pos_embedding\"] = hf_params[\"transformer\"][\"wpe\"][\"embedding\"]\n",
        "\n",
        "    # Copy parameters for each Transformer block\n",
        "    for i in range(n_layer):\n",
        "        hf_block = hf_params[\"transformer\"][\"h\"][str(i)]\n",
        "        block_key = f\"block_{i}\"\n",
        "\n",
        "        # Pre-self-attention LayerNorm (ln_1): Copy bias and scale parameters.\n",
        "        mp[block_key][\"LayerNorm_0\"][\"bias\"] = hf_block[\"ln_1\"][\"bias\"]\n",
        "        mp[block_key][\"LayerNorm_0\"][\"scale\"] = hf_block[\"ln_1\"][\"scale\"]\n",
        "\n",
        "        # Self-Attention:\n",
        "        # For c_attn, transpose the kernel from (3*C, C) to (C, 3*C) and copy the bias.\n",
        "        hf_c_attn_kernel = hf_block[\"attn\"][\"c_attn\"][\"kernel\"]\n",
        "        hf_c_attn_bias = hf_block[\"attn\"][\"c_attn\"][\"bias\"]\n",
        "        mp[block_key][\"attn\"][\"c_attn\"][\"kernel\"] = hf_c_attn_kernel.T\n",
        "        mp[block_key][\"attn\"][\"c_attn\"][\"bias\"] = hf_c_attn_bias\n",
        "\n",
        "        # For c_proj in self-attention, even though the weight matrix is (C, C),\n",
        "        # Apply transpose to ensure correct element ordering, and the bias is copied.\n",
        "        hf_c_proj_kernel = hf_block[\"attn\"][\"c_proj\"][\"kernel\"]\n",
        "        hf_c_proj_bias = hf_block[\"attn\"][\"c_proj\"][\"bias\"]\n",
        "        mp[block_key][\"attn\"][\"c_proj\"][\"kernel\"] = hf_c_proj_kernel.T\n",
        "        mp[block_key][\"attn\"][\"c_proj\"][\"bias\"] = hf_c_proj_bias\n",
        "\n",
        "        # Pre-MLP LayerNorm (ln_2): Copy bias and scale parameters.\n",
        "        mp[block_key][\"LayerNorm_1\"][\"bias\"] = hf_block[\"ln_2\"][\"bias\"]\n",
        "        mp[block_key][\"LayerNorm_1\"][\"scale\"] = hf_block[\"ln_2\"][\"scale\"]\n",
        "\n",
        "        # MLP: For the first linear layer (c_fc), transpose the kernel from (4*C, C) to (C, 4*C) and copy the bias.\n",
        "        c_fc_kernel = hf_block[\"mlp\"][\"c_fc\"][\"kernel\"]\n",
        "        c_fc_bias = hf_block[\"mlp\"][\"c_fc\"][\"bias\"]\n",
        "        mp[block_key][\"ffn\"][\"fc\"][\"kernel\"] = c_fc_kernel.T\n",
        "        mp[block_key][\"ffn\"][\"fc\"][\"bias\"] = c_fc_bias\n",
        "\n",
        "        # For the second linear layer (c_proj) in the MLP, transpose the kernel from (C, 4*C) to (4*C, C) and copy the bias.\n",
        "        c_proj_kernel = hf_block[\"mlp\"][\"c_proj\"][\"kernel\"]\n",
        "        c_proj_bias = hf_block[\"mlp\"][\"c_proj\"][\"bias\"]\n",
        "        mp[block_key][\"ffn\"][\"proj\"][\"kernel\"] = c_proj_kernel.T\n",
        "        mp[block_key][\"ffn\"][\"proj\"][\"bias\"] = c_proj_bias\n",
        "\n",
        "    # Final LayerNorm (ln_f): Copy bias and scale parameters.\n",
        "    mp[\"ln_f\"][\"bias\"] = hf_params[\"transformer\"][\"ln_f\"][\"bias\"]\n",
        "    mp[\"ln_f\"][\"scale\"] = hf_params[\"transformer\"][\"ln_f\"][\"scale\"]\n",
        "\n",
        "    # LM Head: Tie the LM head to the token embeddings by transposing the token embedding.\n",
        "    wte = hf_params[\"transformer\"][\"wte\"][\"embedding\"]  # (vocab_size, n_embd)\n",
        "    mp[\"lm_head\"][\"kernel\"] = wte.T # (n_embd, vocab_size)\n",
        "\n",
        "    return freeze(mp)\n",
        "\n",
        "# Generation Functions\n",
        "def greedy_generate(bound_model, tokenizer, prompt: str, max_new_tokens: int = 30):\n",
        "    \"\"\"\n",
        "    Greedy generation:\n",
        "    - At each step, selects the token with the highest probability.\n",
        "    - Stops generation if the end-of-sequence token is produced.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "    input_ids = jnp.array(input_ids, dtype=jnp.int32)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = bound_model(input_ids)\n",
        "        next_token_logits = logits[:, -1, :]\n",
        "        next_token_id = jnp.argmax(next_token_logits, axis=-1)\n",
        "        input_ids = jnp.concatenate([input_ids, next_token_id[:, None]], axis=1)\n",
        "        # If the EOS token is generated, exit early.\n",
        "        if next_token_id[0] == tokenizer.eos_token_id:\n",
        "            break\n",
        "    output_ids = np.array(input_ids[0])\n",
        "    return tokenizer.decode(output_ids)\n",
        "\n",
        "def sample_generate(bound_model, tokenizer, prompt: str, max_new_tokens: int = 30, temperature: float = 1.0, rng=jax.random.PRNGKey(0)):\n",
        "    \"\"\"\n",
        "    Temperature Sampling:\n",
        "    - At each step, scales logits by temperature and samples from the probability distribution.\n",
        "    - Continues until max_new_tokens are generated or the EOS token is produced.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "    input_ids = jnp.array(input_ids, dtype=jnp.int32)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = bound_model(input_ids)\n",
        "        next_token_logits = logits[:, -1, :] / temperature\n",
        "        next_token_probs = jax.nn.softmax(next_token_logits)\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        next_token_id = jax.random.choice(subkey, next_token_probs.shape[-1], p=next_token_probs[0])\n",
        "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token_id]], dtype=jnp.int32)], axis=1)\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "    output_ids = np.array(input_ids[0])\n",
        "    return tokenizer.decode(output_ids)\n",
        "\n",
        "def top_k_generate(bound_model, tokenizer, prompt: str, max_new_tokens: int = 30, k: int = 50, temperature: float = 1.0, rng=jax.random.PRNGKey(0)):\n",
        "    \"\"\"\n",
        "    Top-k generation:\n",
        "    - At each decoding step, retains only the top k tokens with the highest logits.\n",
        "    - All other token logits are set to a very low value (-1e10) to exclude them from sampling.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "    input_ids = jnp.array(input_ids, dtype=jnp.int32)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = bound_model(input_ids)\n",
        "        next_token_logits = logits[:, -1, :] / temperature\n",
        "        kth_value = jnp.sort(next_token_logits, axis=-1)[:, -k]\n",
        "        filtered_logits = jnp.where(next_token_logits < kth_value[:, None], -1e10, next_token_logits)\n",
        "        next_token_probs = jax.nn.softmax(filtered_logits)\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        next_token_id = jax.random.choice(subkey, next_token_probs.shape[-1], p=next_token_probs[0])\n",
        "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token_id]], dtype=jnp.int32)], axis=1)\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "    output_ids = np.array(input_ids[0])\n",
        "    return tokenizer.decode(output_ids)\n",
        "\n",
        "def top_p_generate(bound_model, tokenizer, prompt: str, max_new_tokens: int = 30,\n",
        "                   top_p: float = 0.9, temperature: float = 1.0, rng=jax.random.PRNGKey(0)):\n",
        "    \"\"\"\n",
        "    Top-p generation:\n",
        "    - At each step, sorts token probabilities in descending order and computes their cumulative sum.\n",
        "    - Retains only the smallest set of tokens whose cumulative probability is within top_p.\n",
        "    - Tokens with probabilities below the threshold are suppressed.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "    input_ids = jnp.array(input_ids, dtype=jnp.int32)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = bound_model(input_ids)\n",
        "        next_token_logits = logits[:, -1, :] / temperature\n",
        "        next_token_probs = jax.nn.softmax(next_token_logits)\n",
        "        sorted_probs = jnp.sort(next_token_probs, axis=-1)[:, ::-1]\n",
        "        sorted_indices = jnp.argsort(next_token_probs, axis=-1)[:, ::-1]\n",
        "        cumulative_probs = jnp.cumsum(sorted_probs, axis=-1)\n",
        "        cutoff = cumulative_probs > top_p\n",
        "        cutoff_indices = jnp.argmax(cutoff, axis=-1)\n",
        "        threshold = sorted_probs.at[jnp.arange(sorted_probs.shape[0]), cutoff_indices].get()\n",
        "        filtered_logits = jnp.where(next_token_probs < threshold[:, None], -1e10, next_token_logits)\n",
        "        next_token_probs = jax.nn.softmax(filtered_logits)\n",
        "        p_1d = next_token_probs[0]\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        next_token_id = jax.random.choice(\n",
        "            subkey,\n",
        "            p_1d.shape[0],\n",
        "            p=p_1d\n",
        "        )\n",
        "        input_ids = jnp.concatenate(\n",
        "            [input_ids, jnp.array([[next_token_id]], dtype=jnp.int32)],\n",
        "            axis=1\n",
        "        )\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "    output_ids = np.array(input_ids[0])\n",
        "    return tokenizer.decode(output_ids)\n",
        "\n",
        "def beam_search_generate(bound_model, tokenizer, prompt: str, beam_width: int = 3, max_new_tokens: int = 30):\n",
        "    \"\"\"\n",
        "    Beam Search:\n",
        "    - At each step, expands each beam by considering the top-k token candidates.\n",
        "    - Retains only the best beam_width sequences based on cumulative log-probabilities.\n",
        "    - Note: This implementation does not support length penalty adjustments.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "    input_ids = jnp.array(input_ids, dtype=jnp.int32)\n",
        "    beams = [(input_ids, 0.0)]\n",
        "    for _ in range(max_new_tokens):\n",
        "        new_beams = []\n",
        "        for seq, score in beams:\n",
        "            logits = bound_model(seq)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            log_probs = jax.nn.log_softmax(next_token_logits)\n",
        "            topk_log_probs, topk_indices = jax.lax.top_k(log_probs, beam_width)\n",
        "            for i in range(beam_width):\n",
        "                new_seq = jnp.concatenate([seq, topk_indices[:, i:i+1]], axis=1)\n",
        "                new_score = score + float(topk_log_probs[0, i])\n",
        "                new_beams.append((new_seq, new_score))\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        if any(b[0][0, -1] == tokenizer.eos_token_id for b in beams):\n",
        "            break\n",
        "    best_seq = beams[0][0]\n",
        "    return tokenizer.decode(np.array(best_seq[0]))\n",
        "\n",
        "\n",
        "# Functions for Comparing Intermediate Layer Outputs\n",
        "# Run the Hugging Face model and retrieve all hidden states.\n",
        "def forward_intermediate_hf(hf_model, input_ids):\n",
        "    outputs = hf_model(input_ids, output_hidden_states=True)\n",
        "    return outputs.hidden_states\n",
        "\n",
        "# Run the custom model and retrieve intermediate outputs from each layer.\n",
        "def forward_intermediate_my(bound_model, input_ids):\n",
        "    return bound_model.forward_intermediate(input_ids, deterministic=True)\n",
        "\n",
        "# Compare and print the L2 norm differences between corresponding hidden states from both models.\n",
        "def compare_intermediate_states(hf_states, my_states):\n",
        "    n = min(len(hf_states), len(my_states))\n",
        "    print(f\"\\n[Compare intermediate states] HF layers: {len(hf_states)}, MY layers: {len(my_states)}\")\n",
        "    for i in range(n):\n",
        "        diff = hf_states[i] - my_states[i]\n",
        "        diff_norm = jnp.linalg.norm(diff)\n",
        "        print(f\"Layer {i}: shape={diff.shape}, L2 diff = {float(diff_norm):.4f}\")\n"
      ],
      "metadata": {
        "id": "E5eCUSuuhDZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "hf_params = hf_model.params\n",
        "\n",
        "my_model = GPT2LMModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_length=1024,  # Default maximum sequence length for GPT-2.\n",
        "    n_embd=768,\n",
        "    n_head=12,\n",
        "    n_layer=12,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "dummy_input_ids = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "variables = my_model.init(rng, dummy_input_ids)\n",
        "my_params = variables[\"params\"]\n",
        "\n",
        "# Convert Hugging Face parameters to the my model format.\n",
        "converted_params = convert_hf_params_to_my_model(hf_params, my_params, n_layer=12)\n",
        "\n",
        "# Bind the converted parameters to my model.\n",
        "bound_model = my_model.bind({\"params\": converted_params})\n",
        "\n",
        "prompt_text = \"I am a dog\"\n",
        "output_greedy = greedy_generate(bound_model, tokenizer, prompt_text)\n",
        "output_sample = sample_generate(bound_model, tokenizer, prompt_text, rng=jax.random.PRNGKey(42))\n",
        "output_top_k = top_k_generate(bound_model, tokenizer, prompt_text, k=40, rng=jax.random.PRNGKey(123))\n",
        "output_top_p = top_p_generate(bound_model, tokenizer, prompt_text, top_p=0.9, rng=jax.random.PRNGKey(321))\n",
        "output_beam = beam_search_generate(bound_model, tokenizer, prompt_text, beam_width=5, max_new_tokens=30)\n",
        "\n",
        "print(\"===== Prompt =====\")\n",
        "print(prompt_text)\n",
        "print(\"===== Greedy Generation =====\")\n",
        "print(output_greedy)\n",
        "print(\"===== Temperature Sampling =====\")\n",
        "print(output_sample)\n",
        "print(\"===== Top-k Generation =====\")\n",
        "print(output_top_k)\n",
        "print(\"===== Top-p Generation =====\")\n",
        "print(output_top_p)\n",
        "print(\"===== Beam Search Generation =====\")\n",
        "print(output_beam)\n",
        "\n",
        "# Compare intermediate layer outputs for validation.\n",
        "test_input_ids = tokenizer(prompt_text, return_tensors=\"np\")[\"input_ids\"]\n",
        "hf_states = forward_intermediate_hf(hf_model, test_input_ids)\n",
        "my_states = forward_intermediate_my(bound_model, test_input_ids)\n",
        "compare_intermediate_states(hf_states, my_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5InFO1kqzCQ",
        "outputId": "e37a89ea-1289-4052-d6b4-18c748d4839b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Prompt =====\n",
            "I am a dog\n",
            "===== Greedy Generation =====\n",
            "I am a dog lover. I love to play with my dog and I love to play with my dog. I love to play with my dog and I love to play\n",
            "===== Temperature Sampling =====\n",
            "I am a dogist and don't like to confront someone for no ludicrous reason. We all have different walks of life and while I believe that some of them do make\n",
            "===== Top-k Generation =====\n",
            "I am a dog lover\", she says, with a laugh. \"So I was really looking forward to watching my dog play, and I had a friend come by the\n",
            "===== Top-p Generation =====\n",
            "I am a dog trainer. To the outside world I am rude and not real nice and to the people inside I am am crazy, disrespectful, cold, domineering\n",
            "===== Beam Search Generation =====\n",
            "I am a dog lover. I love to play with dogs. I love to play with dogs. I love to play with dogs. I love to play with dogs.\n",
            "\n",
            "[Compare intermediate states] HF layers: 13, MY layers: 13\n",
            "Layer 0: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 1: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 2: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 3: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 4: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 5: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 6: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 7: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 8: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 9: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 10: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 11: shape=(1, 4, 768), L2 diff = 0.0000\n",
            "Layer 12: shape=(1, 4, 768), L2 diff = 0.0000\n"
          ]
        }
      ]
    }
  ]
}